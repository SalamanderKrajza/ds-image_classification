# INTRODUCTION
In testing_network_to_reckognize_images.ipynb i was epxloring and playing around some networks to recognize images on new dataset

The main goal is to pick some image data and test how well some network may reckognize it.
In my commercial job I have not practical application for this kind of networks and wanted to experiment with them a little.

Main goal:
- test how well perform network basing on pre-trained layers with known architecture vs creating something from scratch
- trying to analyze trainings and optymize network to improve results
- experiment a little with CNN layers and comparing different architectures implemented in papers

Assumptions:
- I don't wana spent much time on searching for good dataset - if picking just random will generates 100% corret answers i will try something harder, but if there will be room for improvements - i will stick to it
- I am assuming that picking networks trained on milions images and fine-tune them to small dataset shouldn't impact results much even if some smaple may overlap (I am picking different ouptuts so it need to re-learn final classes anwyway)
- I am expecting that data agumentation will be one of best thing to improve results and ignore this fact until very end of project to experiment with different approaches than "more data"
- I am expecting to relatively-easly allow network to find objects from another dataset with this method by using abilities to found features learnt during xception network was trained

I want to compare:
-    **xception_untrained** - Xception network without any training on the new dataset [baseline, probably the worst one]
-    **xception_short_train_unfrozen** - Xception trained for 5 epochs with all layers unfrozen  [it may either - break the existing knowledge or addept better]
-    **xception_short_train_frozen** - Xception trained for 5 epochs with base model layers frozen [i am assuming it may destroy existing knowledge but should be not limited as much]
-    **xception_fine_tuned_from_unfrozen** - Xception fine-tuned with additional epochs, starting from the short-trained model with unfrozen layers and then train with all unfrozen [may be good or bad, depends on previous results]
-    **xception_fine_tuned_from_frozen** - Xception fine-tuned with additional epochs, starting from the short-trained model with frozen layers and then train with all unfrozen [my default workflow in mind]

Then i just went with the flow and test a little more


# TABLE OF CONTENTS (and quick preview of its results)
**INTRODUCTION**

**BASIC PREPARATION**
- connect to drive (for colab), check if GPU available etc.
- Prepare new dataset to train splitted 75%/10%/15%
- Process images to work with network (without data augumentation for now)

**TRAIN FEW VERSIONS OF MODEL BASING ON XCEPTION**
- Test xception with replaced output layer but no traing **[test acc 22%]**
- Train more combinations (5 epochs with/without freezing base model, then 10 epochs with unfreezed)

**SUMMARISE PART**
- Analize metrics on last epoch **[best accuracy on test was 93.09%]**
- Analize accuracy/loss during each epoch for each model
	- Train was still improving with no change on validatoin that suggests overfitting **[reguralization may help]**
	- most has big difference between train/validation which suggests that **[model may lack samples to generalize patterns]**

**ADD REGURALIZATION TO IMPROVE MODEL:**
- Add dropout in between without replacing dense layer **[training was not progressing further and stays on 92.54% acc]**
- Add dropout and replace tuned dense layer with new (reset weights) **[progressing from scratch, after 10 epochs also getting 92.54% acc]**
- Continue training as loss was still improving **[This was still progressing, we get best for now 93,27% and loss was still improving]**
- Try DROPOUT and L2 REGURALIZATION **[New best one - 93,45%]**
- Try DROPOUT and L2 REGURALIZATION without locking layers **[92,54% - wrong direction]**

**MAKE CHANGES TO ARCHITECTURE **
- skip last 13 layers (Assuming network is to complex to generalize) **[New best - 93,81%]**
- skip last 63 layers (Assuming network is to complex to generalize) **[after 40 epochs only 83,09% but still improving relatively fast]**
- skip last 63 layers, try bigger LR (assuming model have not enough time to adapt) **[We are getting close - 91,81%]**

**TRY DIFFERENT ARCHITECTURE (if even cutting-out of half xception is still too complex)**
- inspired by beggining of of xception **[63,9% - looks like this is finally too simple]**
- some basic conv2d layers connected **[70.36% and no progressing]**

**TEST DIFFERENT EXISTING NETWORK ARCHITECTURE - AlexNet**
- test architecture from pytorch (layer sizes mismatch sizes from papers) **[74,36% and visible overfitting]**
- test architecture with correct layersizes and additional batchnormalization (generated by LLM after asking for it) **[72,18%]**

**EXPERIMENT WITH ALEXNET AS BASE TRYING TO IMPROVE GENERALIZATION**
- add bunch of dense layers to see what happen **[56,00% - we are getting much worse]**
- Add more Conv2D layers instead of dense with freezing base **[75,09% - a little improvement but difference between train and validation is just getting bigger]**
- Unfreeze and continue **[77,09% without progressing anymore]**

**FINAL XCEPTION TEST**
- Try XCEPTION architecture without picking pre-trained weights to compare its accuracy **[81,81%]**
- Test if XCEPTION with data augumentation may break 95% accuracy **[96,36% - just as expected, best of all tests]**
